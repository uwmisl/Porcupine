{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.704683Z",
     "start_time": "2020-01-23T01:20:40.178055Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from subprocess import check_output\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "if logger.handlers:\n",
    "    logger.handlers = []\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "\n",
    "# Path for save dir (can be same as f5_dir)\n",
    "output_dir = \"path/to/reads\"\n",
    "try:\n",
    "    os.makedirs(os.path.join(output_dir, \"cnn\"))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Path to fast5 files\n",
    "f5_dir = \"path/to/fast5/files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "\n",
    "# Raw signal data will be extracted from all the fast5s and saved here as input to the classifier\n",
    "molbit_data_file = os.path.join(output_dir, \"molbit_extracted_data.hdf5\")\n",
    "\n",
    "# CNN classification results will be saved here\n",
    "cnn_label_file = os.path.join(output_dir, \"cnn\", \"labeled_reads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.708556Z",
     "start_time": "2020-01-23T01:20:40.706310Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.715112Z",
     "start_time": "2020-01-23T01:20:40.710197Z"
    },
    "code_folding": [
     0,
     18,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def med_mad(data, factor=1.4826):\n",
    "    \"\"\"Modified from Mako.\n",
    "    Compute the Median Absolute Deviation, i.e., the median\n",
    "    of the absolute deviations from the median, and the median.\n",
    "\n",
    "    :param data: A :class:`ndarray` object\n",
    "    :param axis: For multidimensional arrays, which axis to calculate over \n",
    "\n",
    "    :returns: a tuple containing the median and MAD of the data\n",
    "\n",
    "    .. note :: the default `factor` scales the MAD for asymptotically normal\n",
    "        consistency as in R.\n",
    "\n",
    "    \"\"\"\n",
    "    dmed = torch.median(data)\n",
    "    dmad = factor * torch.median(torch.abs(data - dmed))\n",
    "    return dmed, dmad\n",
    "\n",
    "def _scale_data( data):\n",
    "    '''Modified from Mako.'''\n",
    "    med, mad = med_mad(data)\n",
    "    data = (data - med) / mad\n",
    "    return data\n",
    "\n",
    "def trim_start_heuristic(signal, thresh=2, offset=20):\n",
    "    try:\n",
    "        above = np.where(signal[offset:] > thresh)[0][0] + offset\n",
    "    except IndexError:\n",
    "        above = 0\n",
    "    return signal[above:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.728408Z",
     "start_time": "2020-01-23T01:20:40.716871Z"
    },
    "code_folding": [
     0,
     37
    ]
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        O_1, O_2, O_3, O_4, O_5 = 64, 128, 256, 512, 1024\n",
    "        K_1, K_2, K_3, K_4, K_5 = 15, 8, 6, 4, 2\n",
    "        KP_1, KP_2, KP_3, KP_4, KP_5 = 6, 3, 2, 2, 1\n",
    "        FN_1, FN_2 = 1000, 500\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(1, O_1, K_1, stride=1), nn.ReLU(), nn.AvgPool1d(KP_1))\n",
    "        self.conv1_bn = nn.BatchNorm1d(O_1)\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(O_1, O_2, K_2), nn.ReLU(), nn.AvgPool1d(KP_2))\n",
    "        self.conv2_bn = nn.BatchNorm1d(O_2)\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(O_2, O_3, K_3), nn.ReLU(), nn.AvgPool1d(KP_3))\n",
    "        self.conv3_bn = nn.BatchNorm1d(O_3)\n",
    "\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(O_3, O_4, K_4), nn.ReLU(), nn.AvgPool1d(KP_4))\n",
    "        self.conv4_bn = nn.BatchNorm1d(O_4)\n",
    "\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(O_4, O_5, K_5), nn.ReLU(), nn.AvgPool1d(KP_5))\n",
    "        self.conv5_bn = nn.BatchNorm1d(O_5)\n",
    "\n",
    "        # not used, but is in the model file for some reason\n",
    "        self.gru1 = nn.GRU(input_size=92160, hidden_size=10, num_layers=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(37888, FN_1, nn.Dropout(0.5))\n",
    "        self.fc1_bn = nn.BatchNorm1d(FN_1)\n",
    "\n",
    "        self.fc2 = nn.Linear(FN_1, FN_2, nn.Dropout(0.5))\n",
    "        self.fc2_bn = nn.BatchNorm1d(FN_2)\n",
    "\n",
    "        self.fc3 = nn.Linear(FN_2, 96)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1_bn(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv2_bn(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv3_bn(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.relu(self.conv4_bn(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = F.relu(self.conv5_bn(x))\n",
    "        x = x.view(len(x), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1_bn(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc2_bn(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.736672Z",
     "start_time": "2020-01-23T01:20:40.730153Z"
    },
    "code_folding": [
     0,
     1,
     18,
     26,
     29
    ]
   },
   "outputs": [],
   "source": [
    "class MolbitDataset(Dataset):\n",
    "    def __init__(self, data_file, unknown_labels=False):\n",
    "        with h5py.File(data_file, \"r\") as f:\n",
    "            self.data = torch.FloatTensor(f.get(\"data\")[()])\n",
    "            self.n_records = self.data.shape[0]\n",
    "            self.max_len = self.data.shape[2]\n",
    "            try:\n",
    "                self.labels = torch.IntTensor(f.get(\"labels\")[()])\n",
    "                self.n_labels = len(np.unique(self.labels))\n",
    "            except:\n",
    "                self.labels = torch.IntTensor([-1 for _ in range(self.n_records)])\n",
    "                self.n_labels = 0\n",
    "\n",
    "        # Shuffle data\n",
    "        self.shuffle_index = np.random.choice(range(self.n_records), replace=False, size=self.n_records)\n",
    "        self.data = self.data[self.shuffle_index]       \n",
    "        self.labels = self.labels[self.shuffle_index]        \n",
    "        \n",
    "    def _get_onehot(self, label):\n",
    "        if self.labels is None:\n",
    "            return None\n",
    "        ix = self.labels.index(label)\n",
    "        onehot = torch.zeros(self.n_labels)\n",
    "        onehot[ix] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_records\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, :], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.747807Z",
     "start_time": "2020-01-23T01:20:40.738432Z"
    },
    "code_folding": [
     0,
     5,
     23,
     28,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def rescale_counts(counts, scaling_factors):\n",
    "    n_reads = sum(counts)\n",
    "    rescaled = np.multiply(np.array(counts), scaling_factors)\n",
    "    return np.ceil(rescaled / sum(rescaled) * n_reads).astype(int)\n",
    "\n",
    "def get_read_counts(labels, possible_labels=[]):\n",
    "    labels = list(labels)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in possible_labels:\n",
    "            labels[i] = \"-1\"\n",
    "    labels = np.array(labels, dtype=np.array(possible_labels).dtype)\n",
    "    labels, counts = np.unique(labels, return_counts=True)\n",
    "    ordered_counts = np.zeros(len(possible_labels), dtype=int)\n",
    "    for i, possible_label in enumerate(possible_labels):\n",
    "        ix = np.argwhere(labels == possible_label)\n",
    "        if len(ix) > 0:\n",
    "            ix = ix[0][0]\n",
    "            ordered_counts[i] = counts[ix]\n",
    "            assert labels[ix] == possible_label\n",
    "            ordered_counts[i] = counts[ix]\n",
    "    return ordered_counts\n",
    "\n",
    "def get_tag(read_counts, t=0):\n",
    "    read_counts = np.array(read_counts)\n",
    "    tag = list(np.where(read_counts > t, 1, 0))\n",
    "    return tag\n",
    "\n",
    "def decode_c(received_codeword, decoder_binary):\n",
    "    result = check_output([decoder_binary, received_codeword]).decode(\"utf-8\").split(\"\\n\")\n",
    "    codeword_distance = int(re.findall(r\"\\\"distance\\\": ([\\d]+)\", result[2])[0])\n",
    "    corrected_message = re.findall(r\"\\\"message\\\": \\\"([\\d]+)\\\"\", result[1])[0]\n",
    "    return corrected_message, codeword_distance\n",
    "\n",
    "def compute_decoding_helper(counts, decoder_binary, stop_at_d=None):\n",
    "    best_d, best_msg = 99999, None\n",
    "    thresholds = list(np.sort(np.unique(counts)))\n",
    "    for t in thresholds[::-1]:\n",
    "        codeword_at_t = get_tag(counts, t=t)\n",
    "        codeword_str = \"\".join([str(x) for x in codeword_at_t])\n",
    "        closest_msg, closest_d = decode_c(codeword_str, decoder_binary)\n",
    "        if stop_at_d is not None and closest_d <= stop_at_d:\n",
    "            best_d = closest_d\n",
    "            best_msg = closest_msg\n",
    "            break\n",
    "        if closest_d < best_d:\n",
    "            best_d = closest_d\n",
    "            best_msg = closest_msg\n",
    "    return best_d, best_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.766449Z",
     "start_time": "2020-01-23T01:20:40.749435Z"
    },
    "code_folding": [
     0,
     34,
     70,
     76
    ]
   },
   "outputs": [],
   "source": [
    "def decode_run(f5_dir, model_file, decoder_file, possible_labels, molbit_data_file, cnn_label_file,\n",
    "               overwrite=False, conf_thresh=0.9, batch_size=500, n_workers_cnn=30,\n",
    "               scaling_factors=None, decoding_guarantee=None):\n",
    "    \n",
    "    # Read in fast5 data & save to molbit dataset\n",
    "    logger.info(f\"Reading in fast5 data from {f5_dir}.\")\n",
    "    md, read_ids = fast5_to_molbit_dataset(f5_dir, molbit_data_file, overwrite=overwrite)\n",
    "    \n",
    "    # Load saved model\n",
    "    logger.info(f\"Reading in pretrained CNN.\")\n",
    "    model = load_model(model_file)\n",
    "    \n",
    "    # Run classification\n",
    "    logger.info(\"Beginning classification.\")\n",
    "    preds, scores = classify(model, md, conf_thresh=conf_thresh,\n",
    "                             batch_size=batch_size, n_workers=n_workers_cnn)\n",
    "    \n",
    "    # Save predictions to file\n",
    "    logger.info(f\"Saving classifications to {cnn_label_file}.\")\n",
    "    cnn_df = pd.DataFrame()\n",
    "    cnn_df[\"read_id\"] = read_ids\n",
    "    cnn_df[\"cnn_label\"] = preds\n",
    "    cnn_df[\"cnn_score\"] = scores\n",
    "    cnn_df.to_csv(cnn_label_file, sep=\"\\t\", index=False)\n",
    "    \n",
    "    # Decode \n",
    "    logger.info(\"Beginning decoding.\")\n",
    "    d, msg = decode(preds, decoder_file,\n",
    "                    possible_labels=possible_labels,\n",
    "                    scaling_factors=scaling_factors,\n",
    "                    stop_at_d=decoding_guarantee)\n",
    "    logger.info(f\"Decoded: {msg}, {d}\")\n",
    "    return msg, d\n",
    "    \n",
    "def fast5_to_molbit_dataset(f5_dir, molbit_data_file, overwrite=False):\n",
    "    if not os.path.exists(molbit_data_file):\n",
    "        f5_dirs = [f5_dir] + [os.path.join(f5_dir, x) for x in os.listdir(f5_dir) if \"fast5\" in x and not x.endswith(\".fast5\")]\n",
    "        f5_fnames = []\n",
    "        for f5_dir in f5_dirs:\n",
    "            logger.debug(f\"Searching dir: {f5_dir}\")\n",
    "            f5_fnames.extend([os.path.join(f5_dir, x) for x in os.listdir(f5_dir) if x.endswith(\".fast5\")])\n",
    "        try:\n",
    "            assert len(f5_fnames) > 0\n",
    "        except:\n",
    "            print(f\"Checked these directories & found no fast5 files. {f5_dirs}\")\n",
    "        logger.info(f\"Beginning data extraction ({len(f5_fnames)} fast5 files).\")\n",
    "        read_ids = []\n",
    "        signal_data = []\n",
    "        for f5_file in f5_fnames:\n",
    "            with h5py.File(f5_file, \"r\") as f5:\n",
    "                for group in f5.get(\"/\").values():\n",
    "                    read_id = str(dict(group.get(\"Raw\").attrs).get(\"read_id\"))[2:-1]\n",
    "                    read_ids.append(read_id)\n",
    "                    raw = torch.FloatTensor(list(group.get(\"Raw/Signal\")[:15000]))  # extra for normalization\n",
    "                    raw = trim_start_heuristic(_scale_data(raw))[:3000]\n",
    "                    x = torch.zeros(3000)\n",
    "                    x[:len(raw)] = raw\n",
    "                    signal_data.append(x)\n",
    "        signal_data_stacked = torch.stack(signal_data)\n",
    "        signal_data_stacked = signal_data_stacked.unsqueeze(1)\n",
    "        logger.info(f\"Saving to file ({len(read_ids)} reads): {molbit_data_file}\")\n",
    "        with h5py.File(molbit_data_file, \"w\", swmr=True) as f:\n",
    "            f.create_dataset(\"read_ids\", shape=(len(read_ids), ), data=np.array(read_ids, dtype=\"S\"))\n",
    "            f.create_dataset(\"data\", shape=signal_data_stacked.shape, dtype=np.float, data=signal_data_stacked)\n",
    "    else:\n",
    "        with h5py.File(molbit_data_file, \"r\") as f:\n",
    "            read_ids = f.get(\"read_ids\")[()]\n",
    "    md = MolbitDataset(molbit_data_file, unknown_labels=True)\n",
    "    return md, read_ids\n",
    "\n",
    "def load_model(pretrained_model_file):\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(pretrained_model_file))\n",
    "    model.cuda()\n",
    "    return model\n",
    "\n",
    "def classify(model, md, conf_thresh=0.9, batch_size=500, n_workers=30):\n",
    "    assert conf_thresh >= 0 and conf_thresh <= 1\n",
    "    loader_params = {\"batch_size\": batch_size,\n",
    "                     \"num_workers\": n_workers,\n",
    "                     \"shuffle\": False}\n",
    "    data_generator = DataLoader(md, **loader_params)\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for local_batch, local_labels in data_generator:\n",
    "        local_batch = local_batch.to(device)\n",
    "        pred = model(local_batch).to(dtype=torch.float64)\n",
    "        softmax_score = torch.nn.functional.softmax(pred, dim=1)\n",
    "        local_scores, local_preds = torch.max(softmax_score, dim=1)\n",
    "        preds.extend([int(pred) if score > conf_thresh else -1 for score, pred in zip(local_scores.cpu(), local_preds.cpu())])\n",
    "        scores.extend([float(score) for score in local_scores.cpu()])\n",
    "    return preds, scores\n",
    "\n",
    "def decode(labels, decoder_binary, possible_labels=[], scaling_factors=None, stop_at_d=None):\n",
    "    counts = get_read_counts(labels, possible_labels=possible_labels)\n",
    "    if scaling_factors is not None:\n",
    "        counts = rescale_counts(counts, scaling_factors)\n",
    "    best_d, best_msg = compute_decoding_helper(counts, decoder_binary, stop_at_d=stop_at_d)\n",
    "    return best_d, best_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.770659Z",
     "start_time": "2020-01-23T01:20:40.768214Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_label_file = \"test_labels.csv\"\n",
    "model_file = \"../molbit_classification/saved_models/molbit_classification_v4_0_1.20190827.pt\"\n",
    "decoder = \"../ecc/decoder\"\n",
    "possible_labels = range(96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T01:20:40.777705Z",
     "start_time": "2020-01-23T01:20:40.772246Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "scaling_factors = [117.44079692,  296.08219178,   79.3902663 ,   63.8680128 ,\n",
    "        301.24041812,  106.23305345,   50.35782934,   94.36710933,\n",
    "        261.39458779,   23.42805573,  236.19903327,  215.71332122,\n",
    "         72.68457433, 1674.82258065,  359.61558442,   92.43485034,\n",
    "         55.15762106,  147.49710313,  161.68942521,   41.8235584 ,\n",
    "         72.38555587,  124.39775226,  207.99019608,  599.71731449,\n",
    "        410.15625   ,  146.23955432,   81.21546961,  151.60891089,\n",
    "        265.91895803,   93.01442673,   59.58171206,   41.92334018,\n",
    "         75.73638033,  100.18461538,  178.88385542,  176.9227836 ,\n",
    "         35.15      ,   99.06164932,  435.15123095,  124.01737387,\n",
    "        100.70515917,  113.01108647,  127.24327323,   34.53376496,\n",
    "        113.68327138,   86.11075652,  317.00898411,  239.53629243,\n",
    "         83.78780013,  276.0384821 ,   89.75808133,   32.18069662,\n",
    "        250.71262136,  310.93798916,   76.84392204,  187.19391084,\n",
    "        211.31315136,  165.0372093 ,   71.34651475,  403.21590909,\n",
    "         35.59571978,  201.41721854,  126.01242971,   66.43719769,\n",
    "       1425.49333333,  102.0477251 ,   39.45092251,   84.89571202,\n",
    "         68.85702018,  148.00922935,  204.68155712,  104.81568627,\n",
    "         66.45394046,  150.09968354,   32.68883529,   74.21318208,\n",
    "        797.16806723,   93.0257416 ,  348.76102941,  372.37684004,\n",
    "         95.12844828,   56.96902426,  143.82404692,  231.58237146,\n",
    "        171.5491644 ,   65.69370442,   68.64634526,  119.36073553,\n",
    "        128.91764706,   32.27093687,  114.79353994,  433.62242374,\n",
    "         92.13242249,  293.19063545,  129.10751105,   86.49629995]\n",
    "scaling_factors = np.array(scaling_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T18:18:20.707974Z",
     "start_time": "2020-01-23T17:59:10.132512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading in fast5 data from /path/to/data/MinION_sequencing_data_20191216/12_16_19_run_01/12_16_19_run_01/20191216_1931_MN25769_FAL22488_f420bcc0.\n",
      "Searching dir: /path/to/data/MinION_sequencing_data_20191216/12_16_19_run_01/12_16_19_run_01/20191216_1931_MN25769_FAL22488_f420bcc0\n",
      "Searching dir: /path/to/data/MinION_sequencing_data_20191216/12_16_19_run_01/12_16_19_run_01/20191216_1931_MN25769_FAL22488_f420bcc0/fast5\n",
      "Beginning data extraction (102 fast5 files).\n",
      "Saving to file (404696 reads): /path/to/data/MinION_sequencing_data_20191216/guppy_3.2.2_12_16_19_run_01_exec_20200107/test_data.csv\n",
      "Reading in pretrained CNN.\n",
      "Beginning classification.\n",
      "Saving classifications to /path/to/data/MinION_sequencing_data_20191216/guppy_3.2.2_12_16_19_run_01_exec_20200107/cnn/model_v4_0_1_all_reads.csv.\n",
      "Beginning decoding.\n",
      "Decoded: 01001101010010010101001101001100, 9\n"
     ]
    }
   ],
   "source": [
    "best_msg, best_d = decode_run(f5_dir, model_file, decoder, possible_labels, molbit_data_file, cnn_label_file,\n",
    "                              overwrite=False, conf_thresh=0.95, batch_size=500, n_workers_cnn=30,\n",
    "                              scaling_factors=scaling_factors, decoding_guarantee=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true message can gbe given as a bit string (just set true_msg directly) or as a list of molbits\n",
    "molbits_in_set = [1, 2, 3,]  # etc.\n",
    "true_msg = \"\".join([str(x) for x in get_read_counts(molbits_in_set, possible_labels=possible_labels)])[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T18:18:20.731438Z",
     "start_time": "2020-01-23T18:18:20.727443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01001101010010010101001101001100\n",
      "01001101010010010101001101001100\n",
      "00000000000000000000000000000000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(best_msg)\n",
    "print(true_msg)\n",
    "print(\"\".join([\"0\" if s1 == s2 else \"1\" for s1, s2 in zip(best_msg, true_msg)]))\n",
    "print(sum([0 if s1 == s2 else 1 for s1, s2 in zip(best_msg, true_msg)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
